{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow.keras.utils as ku \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Bidirectional, Flatten, Reshape, Dropout","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.202661Z","iopub.execute_input":"2022-08-20T03:28:23.203700Z","iopub.status.idle":"2022-08-20T03:28:23.209940Z","shell.execute_reply.started":"2022-08-20T03:28:23.203653Z","shell.execute_reply":"2022-08-20T03:28:23.208632Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"markdown","source":"Read data from txt file and split it to make it in form of poem","metadata":{}},{"cell_type":"code","source":"data = open('../input/lyrics-generation/lyrics_dataset.txt').read()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.220528Z","iopub.execute_input":"2022-08-20T03:28:23.220793Z","iopub.status.idle":"2022-08-20T03:28:23.229715Z","shell.execute_reply.started":"2022-08-20T03:28:23.220769Z","shell.execute_reply":"2022-08-20T03:28:23.228849Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"corpus = data.lower().split(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.237024Z","iopub.execute_input":"2022-08-20T03:28:23.237362Z","iopub.status.idle":"2022-08-20T03:28:23.246519Z","shell.execute_reply.started":"2022-08-20T03:28:23.237333Z","shell.execute_reply":"2022-08-20T03:28:23.245046Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"Remove duplicates to make unique corpus","metadata":{}},{"cell_type":"code","source":"corpus = list(set(corpus))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.255336Z","iopub.execute_input":"2022-08-20T03:28:23.255621Z","iopub.status.idle":"2022-08-20T03:28:23.261990Z","shell.execute_reply.started":"2022-08-20T03:28:23.255594Z","shell.execute_reply":"2022-08-20T03:28:23.260884Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"Convert text to number using tokenizer by give each word an id","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.272853Z","iopub.execute_input":"2022-08-20T03:28:23.273529Z","iopub.status.idle":"2022-08-20T03:28:23.391784Z","shell.execute_reply.started":"2022-08-20T03:28:23.273490Z","shell.execute_reply":"2022-08-20T03:28:23.390856Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n    \n    input_sequences.append(n_gram_sequence)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.393611Z","iopub.execute_input":"2022-08-20T03:28:23.395063Z","iopub.status.idle":"2022-08-20T03:28:23.508923Z","shell.execute_reply.started":"2022-08-20T03:28:23.395025Z","shell.execute_reply":"2022-08-20T03:28:23.508057Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"input_sequences[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.510176Z","iopub.execute_input":"2022-08-20T03:28:23.511363Z","iopub.status.idle":"2022-08-20T03:28:23.517619Z","shell.execute_reply.started":"2022-08-20T03:28:23.511324Z","shell.execute_reply":"2022-08-20T03:28:23.516649Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"total_words","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.520411Z","iopub.execute_input":"2022-08-20T03:28:23.520957Z","iopub.status.idle":"2022-08-20T03:28:23.527586Z","shell.execute_reply.started":"2022-08-20T03:28:23.520921Z","shell.execute_reply":"2022-08-20T03:28:23.526640Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":"pad sequence by adding zeros to make all lines equal un length","metadata":{}},{"cell_type":"code","source":"max_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences,\n                       maxlen = max_sequence_len, padding='pre'))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.528907Z","iopub.execute_input":"2022-08-20T03:28:23.529581Z","iopub.status.idle":"2022-08-20T03:28:23.558124Z","shell.execute_reply.started":"2022-08-20T03:28:23.529543Z","shell.execute_reply":"2022-08-20T03:28:23.557338Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"markdown","source":"The problem we are solving here is a supervised learning problem. So we will have to provide the model with some labels so that it can generalise the relation between the words used to predict and the predicted word.\n\nSo, we will use our input sequence and use the last word of all sequences as labels for all previous words.","metadata":{}},{"cell_type":"code","source":"train, labels = input_sequences[:,:-1],input_sequences[:,-1]","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.559331Z","iopub.execute_input":"2022-08-20T03:28:23.559848Z","iopub.status.idle":"2022-08-20T03:28:23.567900Z","shell.execute_reply.started":"2022-08-20T03:28:23.559788Z","shell.execute_reply":"2022-08-20T03:28:23.567001Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"labels = ku.to_categorical(labels, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.569244Z","iopub.execute_input":"2022-08-20T03:28:23.569897Z","iopub.status.idle":"2022-08-20T03:28:23.596682Z","shell.execute_reply.started":"2022-08-20T03:28:23.569863Z","shell.execute_reply":"2022-08-20T03:28:23.595840Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words, 50, input_length=max_sequence_len-1))\n# Add an LSTM Layer\nmodel.add(Bidirectional(LSTM(150, return_sequences=True)))  \n# A dropout layer for regularisation\nmodel.add(Dropout(0.2))\n# Add another LSTM Layer\nmodel.add(LSTM(100)) \nmodel.add(Dense(total_words/2, activation='relu'))  \n# In the last layer, the shape should be equal to the total number of words present in our corpus\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:23.598173Z","iopub.execute_input":"2022-08-20T03:28:23.598566Z","iopub.status.idle":"2022-08-20T03:28:24.229208Z","shell.execute_reply.started":"2022-08-20T03:28:23.598527Z","shell.execute_reply":"2022-08-20T03:28:24.228077Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"labels.shape, train.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:24.231071Z","iopub.execute_input":"2022-08-20T03:28:24.231462Z","iopub.status.idle":"2022-08-20T03:28:24.239679Z","shell.execute_reply.started":"2022-08-20T03:28:24.231422Z","shell.execute_reply":"2022-08-20T03:28:24.238647Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"model.fit(train, labels, epochs= 100, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:28:24.243556Z","iopub.execute_input":"2022-08-20T03:28:24.243844Z","iopub.status.idle":"2022-08-20T03:33:49.763076Z","shell.execute_reply.started":"2022-08-20T03:28:24.243794Z","shell.execute_reply":"2022-08-20T03:33:49.761958Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model\n","metadata":{}},{"cell_type":"code","source":"seed_text = \"youssef\"\nnext_words = 90\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict(token_list, verbose=0)\n    output_word = \"\"\n    \n    for word, index in tokenizer.word_index.items():\n        if index == np.argmax(predicted):\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T03:34:20.894798Z","iopub.execute_input":"2022-08-20T03:34:20.895180Z","iopub.status.idle":"2022-08-20T03:34:24.355737Z","shell.execute_reply.started":"2022-08-20T03:34:20.895149Z","shell.execute_reply":"2022-08-20T03:34:24.354766Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}